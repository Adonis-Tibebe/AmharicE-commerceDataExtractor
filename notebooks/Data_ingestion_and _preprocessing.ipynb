{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792ff92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from telethon import TelegramClient\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))  # to import modules from the src/service directory\n",
    "from services.telegram_scrapper import run_scrapper\n",
    "from utils.utils import clean_data, normalize_data, hf_tokenize, regex_tokenize\n",
    "\n",
    "sys.path.append(os.path.abspath('../config'))\n",
    "from config import load_credentials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15132ac",
   "metadata": {},
   "source": [
    "# Loading data from Telegram\n",
    "we will use the **telegram_scrapper** module to load telegram data\n",
    "    - using the **run_scrapper** function(the entry point) of the telegram_scrapper module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheger online-store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Attempt 1 at connecting failed: TimeoutError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data from @Shageronlinestore\n",
      "Shewa Brand\n",
      "Scraped data from @Shewabrand\n",
      "HellooMarket\n",
      "Scraped data from @helloomarketethiopia\n",
      "Fashion tera\n",
      "Scraped data from @Fashiontera\n",
      "NEVA COMPUTER¬Æ\n",
      "Scraped data from @nevacomputer\n",
      "Scrapped Data successfully ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Server resent the older message 7518247682440480769, ignoring\n"
     ]
    }
   ],
   "source": [
    "creds = load_credentials() # load credentials from environment\n",
    "\n",
    "# initializing client\n",
    "client = TelegramClient(\"scraping_session;.session\", creds[\"api_id\"], creds[\"api_hash\"])\n",
    "\n",
    "#await run_scrapper(client) #run the entry point function of the telegram_scrapper \n",
    "print(\"Scrapped Data successfully ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767278d",
   "metadata": {},
   "source": [
    "Data successfully scrapped and stored in the data/raw/ directory we will load and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daddccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/raw/telegram_data.csv\") # read the scrapped data as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aadc67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of messages scrapped: 3000\n",
      "with columns:\n",
      "Channel Title\n",
      "\n",
      "Channel Username\n",
      "\n",
      "ID\n",
      "\n",
      "Message\n",
      "\n",
      "Date\n",
      "\n",
      "Media Path\n",
      "\n",
      "We can see that we have 6 columns‚úÖ\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of messages scrapped: {data.shape[0]}\") # print the total number of messages scrapped\n",
    "print(f\"with columns:\")\n",
    "for col in data.columns:\n",
    "    print(col + \"\\n\")\n",
    "print(f\"We can see that we have {len(data.columns)} columns‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea0b84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel Title          0\n",
       "Channel Username       0\n",
       "ID                     0\n",
       "Message             1223\n",
       "Date                   0\n",
       "Media Path           134\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum() # check for missing values in the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bada3a",
   "metadata": {},
   "source": [
    "We have about 1223 null values in the message which highlights **votes** or other non text messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3ad21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 122 messages in the dataset without photo\n"
     ]
    }
   ],
   "source": [
    "msg_with_no_photo = data[(data[\"Message\"].isna() == False) & (data[\"Media Path\"].isna())].shape[0] # check for messages that have a message but no media path(picture)\n",
    "print(f\"There are {msg_with_no_photo} messages in the dataset without photo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696c41a",
   "metadata": {},
   "source": [
    "# The next task is to clean the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d22232",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_data(data) # clean the data by removing duplicates and messages with no text and media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of messages after cleaning: 1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Channel Title       0\n",
       "Channel Username    0\n",
       "ID                  0\n",
       "Message             0\n",
       "Date                0\n",
       "Media Path          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total number of messages after cleaning: {cleaned_data.shape[0]}\") # print the total number of messages after cleaning\n",
    "cleaned_data.isna().sum() # check for missing values in the cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e3a01",
   "metadata": {},
   "source": [
    "### out data set is cleaned and doesnt contain an **NAN** values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d92ba",
   "metadata": {},
   "source": [
    "## **Data Normalization**:\n",
    "- we will remove all emojis from the messages4\n",
    "- and remove # tags from the message and store in a hashtag column, for if they were to be used for classification downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3903015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel Username</th>\n",
       "      <th>ID</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Media Path</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>7399</td>\n",
       "      <td>GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...</td>\n",
       "      <td>2025-06-20 15:25:41+00:00</td>\n",
       "      <td>../data/raw/photo\\@Shageronlinestore_7399.jpg</td>\n",
       "      <td>[·ãõ·àù_·àû·àç]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>7395</td>\n",
       "      <td>GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...</td>\n",
       "      <td>2025-06-20 15:25:40+00:00</td>\n",
       "      <td>../data/raw/photo\\@Shageronlinestore_7395.jpg</td>\n",
       "      <td>[·ãõ·àù_·àû·àç]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>7393</td>\n",
       "      <td>1L Water Bottle High Quality 1L water time sca...</td>\n",
       "      <td>2025-06-20 11:47:53+00:00</td>\n",
       "      <td>../data/raw/photo\\@Shageronlinestore_7393.jpg</td>\n",
       "      <td>[·ãõ·àù_·àû·àç]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>7391</td>\n",
       "      <td>Sonifer Steam Iron ·ã®·àç·â•·àµ ·àò·â∂·ä®·àª High Quality Cera...</td>\n",
       "      <td>2025-06-20 09:03:23+00:00</td>\n",
       "      <td>../data/raw/photo\\@Shageronlinestore_7391.jpg</td>\n",
       "      <td>[·ãõ·àù_·àû·àç]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sheger online-store</td>\n",
       "      <td>@Shageronlinestore</td>\n",
       "      <td>7390</td>\n",
       "      <td>Sayona multifunctional juicer and extractor Be...</td>\n",
       "      <td>2025-06-20 06:48:11+00:00</td>\n",
       "      <td>../data/raw/photo\\@Shageronlinestore_7390.jpg</td>\n",
       "      <td>[·ãõ·àù_·àû·àç]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>NEVA COMPUTER¬Æ</td>\n",
       "      <td>@nevacomputer</td>\n",
       "      <td>8105</td>\n",
       "      <td>This Alienware m15 R5 Ryzen Edition Gaming Lap...</td>\n",
       "      <td>2023-11-28 12:27:58+00:00</td>\n",
       "      <td>../data/raw/photo\\@nevacomputer_8105.jpg</td>\n",
       "      <td>[no tag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>NEVA COMPUTER¬Æ</td>\n",
       "      <td>@nevacomputer</td>\n",
       "      <td>8103</td>\n",
       "      <td>The Alienware m15 Ryzen Edition R5 is engineer...</td>\n",
       "      <td>2023-11-28 12:15:47+00:00</td>\n",
       "      <td>../data/raw/photo\\@nevacomputer_8103.jpg</td>\n",
       "      <td>[no tag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>NEVA COMPUTER¬Æ</td>\n",
       "      <td>@nevacomputer</td>\n",
       "      <td>8102</td>\n",
       "      <td>Dell Alienware M15 R5 15.6'' QHD Gaming Laptop...</td>\n",
       "      <td>2023-11-28 12:13:18+00:00</td>\n",
       "      <td>../data/raw/photo\\@nevacomputer_8102.jpg</td>\n",
       "      <td>[no tag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>NEVA COMPUTER¬Æ</td>\n",
       "      <td>@nevacomputer</td>\n",
       "      <td>8099</td>\n",
       "      <td>NEW ARRIVAL from BRAND : Dell inspiron DISPLAY...</td>\n",
       "      <td>2023-11-28 05:55:47+00:00</td>\n",
       "      <td>../data/raw/photo\\@nevacomputer_8099.jpg</td>\n",
       "      <td>[no tag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>NEVA COMPUTER¬Æ</td>\n",
       "      <td>@nevacomputer</td>\n",
       "      <td>8092</td>\n",
       "      <td>COMPUTER offer BRAND : HP pavilion DISPLAY: 14...</td>\n",
       "      <td>2023-11-28 05:50:59+00:00</td>\n",
       "      <td>../data/raw/photo\\@nevacomputer_8092.jpg</td>\n",
       "      <td>[NEVA, student]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1777 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Channel Title    Channel Username    ID  \\\n",
       "0     Sheger online-store  @Shageronlinestore  7399   \n",
       "1     Sheger online-store  @Shageronlinestore  7395   \n",
       "2     Sheger online-store  @Shageronlinestore  7393   \n",
       "3     Sheger online-store  @Shageronlinestore  7391   \n",
       "4     Sheger online-store  @Shageronlinestore  7390   \n",
       "...                   ...                 ...   ...   \n",
       "1772       NEVA COMPUTER¬Æ       @nevacomputer  8105   \n",
       "1773       NEVA COMPUTER¬Æ       @nevacomputer  8103   \n",
       "1774       NEVA COMPUTER¬Æ       @nevacomputer  8102   \n",
       "1775       NEVA COMPUTER¬Æ       @nevacomputer  8099   \n",
       "1776       NEVA COMPUTER¬Æ       @nevacomputer  8092   \n",
       "\n",
       "                                                Message  \\\n",
       "0     GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...   \n",
       "1     GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...   \n",
       "2     1L Water Bottle High Quality 1L water time sca...   \n",
       "3     Sonifer Steam Iron ·ã®·àç·â•·àµ ·àò·â∂·ä®·àª High Quality Cera...   \n",
       "4     Sayona multifunctional juicer and extractor Be...   \n",
       "...                                                 ...   \n",
       "1772  This Alienware m15 R5 Ryzen Edition Gaming Lap...   \n",
       "1773  The Alienware m15 Ryzen Edition R5 is engineer...   \n",
       "1774  Dell Alienware M15 R5 15.6'' QHD Gaming Laptop...   \n",
       "1775  NEW ARRIVAL from BRAND : Dell inspiron DISPLAY...   \n",
       "1776  COMPUTER offer BRAND : HP pavilion DISPLAY: 14...   \n",
       "\n",
       "                           Date  \\\n",
       "0     2025-06-20 15:25:41+00:00   \n",
       "1     2025-06-20 15:25:40+00:00   \n",
       "2     2025-06-20 11:47:53+00:00   \n",
       "3     2025-06-20 09:03:23+00:00   \n",
       "4     2025-06-20 06:48:11+00:00   \n",
       "...                         ...   \n",
       "1772  2023-11-28 12:27:58+00:00   \n",
       "1773  2023-11-28 12:15:47+00:00   \n",
       "1774  2023-11-28 12:13:18+00:00   \n",
       "1775  2023-11-28 05:55:47+00:00   \n",
       "1776  2023-11-28 05:50:59+00:00   \n",
       "\n",
       "                                         Media Path         hashtags  \n",
       "0     ../data/raw/photo\\@Shageronlinestore_7399.jpg          [·ãõ·àù_·àû·àç]  \n",
       "1     ../data/raw/photo\\@Shageronlinestore_7395.jpg          [·ãõ·àù_·àû·àç]  \n",
       "2     ../data/raw/photo\\@Shageronlinestore_7393.jpg          [·ãõ·àù_·àû·àç]  \n",
       "3     ../data/raw/photo\\@Shageronlinestore_7391.jpg          [·ãõ·àù_·àû·àç]  \n",
       "4     ../data/raw/photo\\@Shageronlinestore_7390.jpg          [·ãõ·àù_·àû·àç]  \n",
       "...                                             ...              ...  \n",
       "1772       ../data/raw/photo\\@nevacomputer_8105.jpg         [no tag]  \n",
       "1773       ../data/raw/photo\\@nevacomputer_8103.jpg         [no tag]  \n",
       "1774       ../data/raw/photo\\@nevacomputer_8102.jpg         [no tag]  \n",
       "1775       ../data/raw/photo\\@nevacomputer_8099.jpg         [no tag]  \n",
       "1776       ../data/raw/photo\\@nevacomputer_8092.jpg  [NEVA, student]  \n",
       "\n",
       "[1777 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_normalized_data = normalize_data(cleaned_data)\n",
    "cleaned_normalized_data.reset_index(drop=True, inplace=True)\n",
    "cleaned_normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343f7905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Channel Title', 'Channel Username', 'ID', 'Message', 'Date',\n",
      "       'Media Path', 'hashtags'],\n",
      "      dtype='object')\n",
      "we have added a new 'hashtags' column to the dataframe‚úÖ\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_normalized_data.columns)\n",
    "print(f\"we have added a new '{cleaned_normalized_data.columns[-1]}' column to the dataframe‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5be83",
   "metadata": {},
   "source": [
    "# Data Tokenization \n",
    "- For initial tokenization we'll use a regx based tokenizer since it is easier to add custome tags\n",
    "- we will store the tokenized text on a tokenized_msg column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435f5dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized messages using regex tokenizer ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "cleaned_normalized_data[\"tokenized_msg\"] = cleaned_normalized_data[\"Message\"].apply(regex_tokenize) # tokenize the messages using regex tokenizer\n",
    "print(\"Tokenized messages using regex tokenizer ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c5cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_msg</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[GROOMING, SET, ·à∂·àµ·âµ, ·â†·ä†·äï·ãµ, ·ã®·ã´·ãò, ·ã®·çÄ·åâ·à≠, ·àõ·àΩ·äï, ·ä•·äì,...</td>\n",
       "      <td>GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[GROOMING, SET, ·à∂·àµ·âµ, ·â†·ä†·äï·ãµ, ·ã®·ã´·ãò, ·ã®·çÄ·åâ·à≠, ·àõ·àΩ·äï, ·ä•·äì,...</td>\n",
       "      <td>GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1L, Water, Bottle, High, Quality, 1L, water, ...</td>\n",
       "      <td>1L Water Bottle High Quality 1L water time sca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Sonifer, Steam, Iron, ·ã®·àç·â•·àµ, ·àò·â∂·ä®·àª, High, Quali...</td>\n",
       "      <td>Sonifer Steam Iron ·ã®·àç·â•·àµ ·àò·â∂·ä®·àª High Quality Cera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Sayona, multifunctional, juicer, and, extract...</td>\n",
       "      <td>Sayona multifunctional juicer and extractor Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[2in1, long, handled, bath, brush, ·àà·ä†·ã´·ã´·ãù, ·àù·âπ, ...</td>\n",
       "      <td>2in1 long handled bath brush ·àà·ä†·ã´·ã´·ãù ·àù·âπ ·â†·âÄ·àã·àâ ·ã®·àõ·äï...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Miralux, Hot, plate, ·â£·àà·àÅ·àà·âµ, ·àù·ãµ·åÉ, ·àµ·â∂·â≠, orginal...</td>\n",
       "      <td>Miralux Hot plate ·â£·àà·àÅ·àà·âµ ·àù·ãµ·åÉ ·àµ·â∂·â≠ orginal 2000 ·ãã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[7pcs, glass, water, set, ·ä†·äï·ãµ, ·àõ·à´·ä™, ·åÜ·åç·äì, 6, ·àò·å†...</td>\n",
       "      <td>7pcs glass water set ·ä†·äï·ãµ ·àõ·à´·ä™ ·åÜ·åç·äì 6 ·àò·å†·å´ ·â•·à≠·å≠·âÜ·ãé·âΩ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Universal, water, -, saving, dishwasher, head...</td>\n",
       "      <td>Universal water-saving dishwasher head Increas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[special, base, for, refrigerator, and, washin...</td>\n",
       "      <td>special base for refrigerator and washing mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[special, base, for, refrigerator, and, washin...</td>\n",
       "      <td>special base for refrigerator and washing mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Vintage, Mason, Glass, Drinking, Jar, with, S...</td>\n",
       "      <td>Vintage Mason Glass Drinking Jar with Straw ·ã®·à´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[HOBBY, LOBBY, 3, in, 1, Chopper, and, Garlic,...</td>\n",
       "      <td>HOBBY LOBBY 3 in 1 Chopper and Garlic Peeler ·ä•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[HOBBY, LOBBY, 3, in, 1, Chopper, and, Garlic,...</td>\n",
       "      <td>HOBBY LOBBY 3 in 1 Chopper and Garlic Peeler ·ä•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[HOBBY, LOBBY, 3, in, 1, Chopper, and, Garlic,...</td>\n",
       "      <td>HOBBY LOBBY 3 in 1 Chopper and Garlic Peeler ·ä•...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokenized_msg  \\\n",
       "0   [GROOMING, SET, ·à∂·àµ·âµ, ·â†·ä†·äï·ãµ, ·ã®·ã´·ãò, ·ã®·çÄ·åâ·à≠, ·àõ·àΩ·äï, ·ä•·äì,...   \n",
       "1   [GROOMING, SET, ·à∂·àµ·âµ, ·â†·ä†·äï·ãµ, ·ã®·ã´·ãò, ·ã®·çÄ·åâ·à≠, ·àõ·àΩ·äï, ·ä•·äì,...   \n",
       "2   [1L, Water, Bottle, High, Quality, 1L, water, ...   \n",
       "3   [Sonifer, Steam, Iron, ·ã®·àç·â•·àµ, ·àò·â∂·ä®·àª, High, Quali...   \n",
       "4   [Sayona, multifunctional, juicer, and, extract...   \n",
       "5   [2in1, long, handled, bath, brush, ·àà·ä†·ã´·ã´·ãù, ·àù·âπ, ...   \n",
       "6   [Miralux, Hot, plate, ·â£·àà·àÅ·àà·âµ, ·àù·ãµ·åÉ, ·àµ·â∂·â≠, orginal...   \n",
       "7   [7pcs, glass, water, set, ·ä†·äï·ãµ, ·àõ·à´·ä™, ·åÜ·åç·äì, 6, ·àò·å†...   \n",
       "8   [Universal, water, -, saving, dishwasher, head...   \n",
       "9   [special, base, for, refrigerator, and, washin...   \n",
       "10  [special, base, for, refrigerator, and, washin...   \n",
       "11  [Vintage, Mason, Glass, Drinking, Jar, with, S...   \n",
       "12  [HOBBY, LOBBY, 3, in, 1, Chopper, and, Garlic,...   \n",
       "13  [HOBBY, LOBBY, 3, in, 1, Chopper, and, Garlic,...   \n",
       "14  [HOBBY, LOBBY, 3, in, 1, Chopper, and, Garlic,...   \n",
       "\n",
       "                                              Message  \n",
       "0   GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...  \n",
       "1   GROOMING SET ·à∂·àµ·âµ ·â†·ä†·äï·ãµ ·ã®·ã´·ãò ·ã®·çÄ·åâ·à≠ ·àõ·àΩ·äï ·ä•·äì ·àº·â®·à≠ ·ã®·àö·à∞·à´...  \n",
       "2   1L Water Bottle High Quality 1L water time sca...  \n",
       "3   Sonifer Steam Iron ·ã®·àç·â•·àµ ·àò·â∂·ä®·àª High Quality Cera...  \n",
       "4   Sayona multifunctional juicer and extractor Be...  \n",
       "5   2in1 long handled bath brush ·àà·ä†·ã´·ã´·ãù ·àù·âπ ·â†·âÄ·àã·àâ ·ã®·àõ·äï...  \n",
       "6   Miralux Hot plate ·â£·àà·àÅ·àà·âµ ·àù·ãµ·åÉ ·àµ·â∂·â≠ orginal 2000 ·ãã...  \n",
       "7   7pcs glass water set ·ä†·äï·ãµ ·àõ·à´·ä™ ·åÜ·åç·äì 6 ·àò·å†·å´ ·â•·à≠·å≠·âÜ·ãé·âΩ ...  \n",
       "8   Universal water-saving dishwasher head Increas...  \n",
       "9   special base for refrigerator and washing mach...  \n",
       "10  special base for refrigerator and washing mach...  \n",
       "11  Vintage Mason Glass Drinking Jar with Straw ·ã®·à´...  \n",
       "12  HOBBY LOBBY 3 in 1 Chopper and Garlic Peeler ·ä•...  \n",
       "13  HOBBY LOBBY 3 in 1 Chopper and Garlic Peeler ·ä•...  \n",
       "14  HOBBY LOBBY 3 in 1 Chopper and Garlic Peeler ·ä•...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_normalized_data[[\"tokenized_msg\", \"Message\"]].head(15)# print the first 15 rows of the tokenized messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cffc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and normalized data saved to ../data/processed/cleaned_normalized_data.csv ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# save the cleaned and normalized data to a csv file for downstream tasks\n",
    "cleaned_normalized_data.to_csv(\"../data/processed/cleaned_normalized_data.csv\", index=False)\n",
    "print(\"Cleaned and normalized data saved to ../data/processed/cleaned_normalized_data.csv ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353ebe1",
   "metadata": {},
   "source": [
    "# üìí Data Ingestion and Preprocessing Summary\n",
    "\n",
    "This notebook outlines the pipeline for collecting and preparing Telegram data for analysis, focusing on Amharic-language e-commerce channels.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Loading Data from Telegram\n",
    "\n",
    "- Uses a custom `telegram_scrapper` module and the `Telethon` library.\n",
    "- Connects to Telegram using API credentials.\n",
    "- Scrapes messages, images, and documents from relevant channels.\n",
    "- Stores raw data in `data/raw/telegram_data.csv`.\n",
    "- stores photos in the same directory as the rew telegram_data.csv in a folder called photo\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Exploration\n",
    "\n",
    "- Loads the raw data into a Pandas DataFrame.\n",
    "- Explores the dataset structure and checks for missing values.\n",
    "- Identifies entries withoug messages which might be polls or other non text messages.\n",
    "- Identifies messages with and without media attachments.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Cleaning\n",
    "\n",
    "- Removes duplicates and irrelevant entries (e.g., empty messages).\n",
    "- Ensures the cleaned dataset contains no missing values.\n",
    "\n",
    "**Note**: Entries with a message but no media entries are not removed as they are needed for the NER pipeline\n",
    "---\n",
    "\n",
    "## 4. Data Normalization\n",
    "\n",
    "- Removes emojis from messages.\n",
    "- Extracts hashtags and stores them in a separate column for downstream tasks.\n",
    "- Normalizes Negative circled characters which might cause noise later downstream to their base textual form\n",
    "- Normalize whitespaces for smoother tokenization\n",
    "- Normalize non-breaking spaces that appear as /ax0 in the text to regular spaces\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Data Tokenization\n",
    "\n",
    "- Applies a regex-based tokenizer to the message text.\n",
    "- Stores tokenized output in a new `tokenized_msg` column.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Save preprocessed data\n",
    "- store preprocessed data to the `..data/processed/cleaned_normalized_data.csv`\n",
    "\n",
    "**Result:**  \n",
    "A clean, normalized, and tokenized dataset ready leabeling and model fine tuning, with metadata and message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
